## **Eats-Guide **
Developing an AI guide using LLMs and Vector DBs to retrieve data.

This project focuses on developing and enhancing a system that combines **large language models (LLMs)** with **vector databases (vectordb)** for efficient retrieval and generation of context-aware responses. By leveraging embeddings and retrieval-augmented generation techniques, 
the goal is to enhance the quality and relevance of responses generated by the LLM.

---
## **Project overview**
![Project architecture](resources/proposed_arch1.jpeg)

## **Demo Video**

To see the system in action, watch the demo video below:

https://drive.google.com/file/d/16e2FOA8XY6ZGp-rCVRuKlA4cq_Mp3Ws_/view?usp=sharing 



---
##  **Version 1: Initial Implementation**

In **Version 1**, the workflow begins by converting the dataset into **text embeddings** and storing them in the **vectordb**. When a user provides a prompt:

1. **Text embeddings** are generated and used to **search the vector database** for the most relevant information.
2. The **retrieved data** is then passed to the **LLM**, which uses this information to generate a response.
3. This approach separates the **retrieval** and **generation** tasks, focusing on augmenting LLM outputs with relevant data from vectordb.

---

##  **Version 2: Enhanced Integration** (Current Version)

In **Version 2**, a more streamlined approach is adopted, where the **LLM** directly integrates with the **vectordb** during response generation:

1. The dataset is preprocessed into **text embeddings** and stored in the **vectordb**.
2. Upon receiving a user prompt, **text embeddings** are generated and used to query the **vectordb** for the most relevant data.
3. The **LLM** conditions its response based on both the **retrieved information** and the **user prompt**, resulting in a more context-aware and relevant output.

This version improves the efficiency of the system by directly involving the **LLM** earlier in the process, leading to more **informed and relevant responses**.

---
---

