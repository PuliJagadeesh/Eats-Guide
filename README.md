## **AI_Guide**
Developing an AI guide using LLMs and Vector Dbs to retrieve data.



This project focuses on developing and enhancing a system that combines **large language models (LLMs)** with **vector databases (vectordb)** for efficient retrieval and generation of context-aware responses. By leveraging embeddings and retrieval-augmented generation techniques, 
the goal is to enhance the quality and relevance of responses generated by the LLM.

---
## **Project overview**
![Project architecture](resources/proposed_arch.jpg)


##  **Version 1: Initial Implementation**

In **Version 1**, the workflow begins by converting the dataset into **text embeddings** and storing them in the **vectordb**. When a user provides a prompt:

1. **Text embeddings** are generated and used to **search the vector database** for the most relevant information.
2. The **retrieved data** is then passed to the **LLM**, which uses this information to generate a response.
3. This approach separates the **retrieval** and **generation** tasks, focusing on augmenting LLM outputs with relevant data from vectordb.

---

##  **Version 2: Enhanced Integration** (Current Version)

In **Version 2**, a more streamlined approach is adopted, where the **LLM** directly integrates with the **vectordb** during response generation:

1. The dataset is preprocessed into **text embeddings** and stored in the **vectordb**.
2. Upon receiving a user prompt, **text embeddings** are generated and used to query the **vectordb** for the most relevant data.
3. The **LLM** conditions its response based on both the **retrieved information** and the **user prompt**, resulting in a more context-aware and relevant output.

This version improves the efficiency of the system by directly involving the **LLM** earlier in the process, leading to more **informed and relevant responses**.

---

## **Version History**

| **Version** | **Date Updated** | **Summary** |
|-------------|------------------|-------------|
| **Version 1**   | *[15-10-24]*        | This approach separates the **retrieval** and **generation** tasks, focusing on augmenting LLM outputs with relevant data from vectordb. |
| **Version 2**   | *[18-10-24]*        | The **LLM** conditions its response based on both the **retrieved information** and the **user prompt**, resulting in a more context-aware and relevant output |
| **Version 3**   | *[]*                | work in progress  |



---

